{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Practical Deep Learning, Lesson 4, Language Model Blog Post Imitator\"\n",
    "date: 2024-11-04T13:57:00-04:00\n",
    "draft: false\n",
    "tags:\n",
    "- language_models\n",
    "- course.fast.ai\n",
    "series: \"Fast.ai Course\"\n",
    "github_url: https://github.com/danielcorin/fastbook_projects/tree/main/blog_post_imitator\n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook/post, we're going to be using the markdown content from my [blog](https://github.com/danielcorin/blog) to try a language model.\n",
    "From this, we'll attempt to prompt the model to generate a post for a topic I might write about.\n",
    "\n",
    "Let's import `fastai` and disable warnings since these pollute the notebook a lot when I'm trying to convert these notebooks into posts (I am writing this as a notebook and converting it to a markdown file with [this script](https://github.com/danielcorin/blog/blob/8181116943a7e4a8583edcf9d64c2b08b41cbf34/scripts/convert_notebook.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The written content in my blog is markdown (.md) files.\n",
    "You can see the raw contents of any of these posts by appending `/index.md` to the end of the URL on any post on this site.\n",
    "They look something like\n",
    "\n",
    "```md\n",
    "---\n",
    "<frontmatter key values>\n",
    "---\n",
    "\n",
    "<the rest of the post with code, links, images, shortcodes, etc.>\n",
    "```\n",
    "\n",
    "To get started, I modeled my approach after the one used in [chapter 10](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb), which looks something like\n",
    "\n",
    "```python\n",
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ").dataloaders(path, path=path, bs=128, seq_len=80)\n",
    "```\n",
    "\n",
    "There were a few modifications I needed to make with the approach.\n",
    "For starters, we were loading `.md` files rather than text files, so initially I tried to do this with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/content/posts/2013/2013-07-05-qc.md\n",
      "data/content/posts/2024/models-writing-about-coding-with-models.md\n",
      "data/content/posts/2024/vlms-hallucinate.md\n"
     ]
    }
   ],
   "source": [
    "path = Path(\"./data/content\")\n",
    "files = get_files(path, extensions='.md', recurse=True)\n",
    "for f in files[:3]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these seemed to cause opaque and confusing issues with `DataBlock` or `DataLoaders` that manifested something like this\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "Cell In[209], line 10\n",
    "      3 # First, create a tokenizer\n",
    "      4 text_processor = TextBlock.from_folder(path, is_lm=True)\n",
    "      6 dls_lm = DataBlock(\n",
    "      7     blocks=text_processor,\n",
    "      8     get_items=get,\n",
    "      9     splitter=RandomSplitter(0.1)\n",
    "---> 10 ).dataloaders(\n",
    "     11     path,\n",
    "     12     path=path,\n",
    "     13     bs=128,\n",
    "     14     seq_len=80,\n",
    "     15 )\n",
    "\n",
    "File ~/dev/lab/fastbook_projects/blog_post_generator/.venv/lib/python3.12/site-packages/fastai/data/block.py:157, in DataBlock.dataloaders(self, source, path, verbose, **kwargs)\n",
    "    151 def dataloaders(self,\n",
    "    152     source, # The data source\n",
    "    153     path:str='.', # Data source and default `Learner` path\n",
    "    154     verbose:bool=False, # Show verbose messages\n",
    "    155     **kwargs\n",
    "    156 ) -> DataLoaders:\n",
    "--> 157     dsets = self.datasets(source, verbose=verbose)\n",
    "    158     kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "...\n",
    "    387     self.types.append(type(x))\n",
    "--> 388 types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
    "    389 self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
    "\n",
    "TypeError: 'NoneType' object is not iterable\n",
    "```\n",
    "\n",
    "To workaround this challenge, I changed all the file extensions to `.txt`.\n",
    "This allowed the model to load and tokenize the dataset.\n",
    "\n",
    "Next, I had an issue with encoding\n",
    "\n",
    "```\n",
    "    return f.read()\n",
    "           ^^^^^^^^\n",
    "  File \"<frozen codecs>\", line 322, in decode\n",
    "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
    "```\n",
    "\n",
    "I solved this in the `copy_and_rename_files` function with\n",
    "\n",
    "```python\n",
    "src_file.read_text(encoding=encoding)\n",
    "```\n",
    "\n",
    "There is also a function called `clean_content` which I will address later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_content(text):\n",
    "    # Handle code blocks with language specifiers\n",
    "    text = re.sub(r'```(\\w+)?\\n(.*?)```', lambda m: f'<CODE>{m.group(2)}</CODE>', text, flags=re.DOTALL)\n",
    "\n",
    "    # Replace single backticks\n",
    "    text = re.sub(r'`([^`]+)`', r'<INLINE_CODE>\\1</INLINE_CODE>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def copy_and_rename_files():\n",
    "    src_dir = Path(\"./data/content\")\n",
    "    dst_dir = Path(\"./data/cleaned_content\")\n",
    "\n",
    "    if not dst_dir.exists():\n",
    "        dst_dir.mkdir(parents=True)\n",
    "\n",
    "    for src_file in src_dir.rglob(\"*.md\"):\n",
    "        try:\n",
    "            content = None\n",
    "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "                try:\n",
    "                    content = src_file.read_text(encoding=encoding)\n",
    "                    if content.startswith('---'):\n",
    "                        # Remove markdown frontmatter\n",
    "                        parts = content.split('---', 2)\n",
    "                        if len(parts) >= 3:\n",
    "                            content = parts[2]\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "            if content is None:\n",
    "                print(f\"Skipping {src_file}: Unable to decode with supported encodings\")\n",
    "                continue\n",
    "\n",
    "            content = clean_content(content)\n",
    "\n",
    "            rel_path = src_file.relative_to(src_dir)\n",
    "            dst_file = dst_dir / rel_path.with_suffix('.txt')\n",
    "            dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            dst_file.write_text(content, encoding='utf-8')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {src_file}: {str(e)}\")\n",
    "\n",
    "copy_and_rename_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "With the needed adjustments to the dataset made, and the model-ready content now in the `data/cleaned_content` folder, we can load and tokenize that data with `fastai`.\n",
    "\n",
    "We validate that we can read the files paths with our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned_content/posts/2013/2013-07-05-qc.txt\n",
      "data/cleaned_content/posts/2024/language-model-based-aggregators.txt\n",
      "data/cleaned_content/posts/2024/making-your-vision-real.txt\n"
     ]
    }
   ],
   "source": [
    "path = Path(\"./data/cleaned_content\")\n",
    "files = get_text_files(path)\n",
    "for f in files[:3]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a `DataBlock` and view the loaded, tokenized content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get = partial(get_text_files, folders=['posts', 'til', 'logs', 'projects'])\n",
    "\n",
    "text_processor = TextBlock.from_folder(path, is_lm=True)\n",
    "\n",
    "dls_lm = DataBlock(\n",
    "    blocks=text_processor,\n",
    "    get_items=get,\n",
    "    splitter=RandomSplitter(0.1)\n",
    ").dataloaders(\n",
    "    path,\n",
    "    path=path,\n",
    "    bs=128,\n",
    "    seq_len=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj i 've been wanting to create a chat component for this site for a while , because i really do n't like quoting conversations and manually formatting them each time . \\n xxmaj when using a model playground , usually there is a code snippet option that generates xxmaj python code you can copy out intro a script . \\n xxmaj using that feature , i can now copy the message list and paste it as xxup json into a xxmaj hugo shortcode and get results like this : \\n\\n\\n { { &lt; chat model=\"gpt-4o - mini \" &gt; } } \\n [ \\n▁ { \\n▁ \" role \" : \" system \" , \\n▁ \" content \" : [ \\n▁ { \\n▁ \" type \" : \" text \" , \\n▁ \" text \" : \" you should respond with the understanding they are an experienced software</td>\n",
       "      <td>xxmaj i 've been wanting to create a chat component for this site for a while , because i really do n't like quoting conversations and manually formatting them each time . \\n xxmaj when using a model playground , usually there is a code snippet option that generates xxmaj python code you can copy out intro a script . \\n xxmaj using that feature , i can now copy the message list and paste it as xxup json into a xxmaj hugo shortcode and get results like this : \\n\\n\\n { { &lt; chat model=\"gpt-4o - mini \" &gt; } } \\n [ \\n▁ { \\n▁ \" role \" : \" system \" , \\n▁ \" content \" : [ \\n▁ { \\n▁ \" type \" : \" text \" , \\n▁ \" text \" : \" you should respond with the understanding they are an experienced software engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxunk / aka \\n [ sublime xxup cli ] : https : / / xxrep 3 w xxunk / docs / 2 / xxunk xxbos i built a [ site](https : / / xxunk / ) to host a language model generated kid ’s book i built using chatgpt and xxmaj midjourney . xxmaj the plot was sourced from my book xxunk to see how a language model would perform writing a xxunk with an unusual plot . \\n xxmaj it prompted some interesting conversations about the role of xxunk and art in culture on the [ xxunk xxunk : / / news.ycombinator.com / xxunk ) . \\n\\n xxmaj tech : xxmaj react , xxmaj next.js , openai , xxmaj midjourney , xxmaj vercel \\n\\n [ source code](https : / / github.com / danielcorin / adventure - of - xxunk / ) xxbos i wrote a tiny site to use</td>\n",
       "      <td>/ aka \\n [ sublime xxup cli ] : https : / / xxrep 3 w xxunk / docs / 2 / xxunk xxbos i built a [ site](https : / / xxunk / ) to host a language model generated kid ’s book i built using chatgpt and xxmaj midjourney . xxmaj the plot was sourced from my book xxunk to see how a language model would perform writing a xxunk with an unusual plot . \\n xxmaj it prompted some interesting conversations about the role of xxunk and art in culture on the [ xxunk xxunk : / / news.ycombinator.com / xxunk ) . \\n\\n xxmaj tech : xxmaj react , xxmaj next.js , openai , xxmaj midjourney , xxmaj vercel \\n\\n [ source code](https : / / github.com / danielcorin / adventure - of - xxunk / ) xxbos i wrote a tiny site to use to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, so we create a learner then run the same approach as is done in Chapter 10, checkpointing as we go.\n",
    "We don't _have_ to do it this way -- we could just call `fit_one_cycle` the number of times we want -- but it was helpful for me to validate the process end-to-end once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
    "    metrics=[accuracy, Perplexity()]\n",
    ").to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.923746</td>\n",
       "      <td>4.835233</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>125.867935</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('data/cleaned_content/models/1epoch.pth')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load('1epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the bulk of the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.721324</td>\n",
       "      <td>4.359152</td>\n",
       "      <td>0.278559</td>\n",
       "      <td>78.190788</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.582399</td>\n",
       "      <td>3.972712</td>\n",
       "      <td>0.338368</td>\n",
       "      <td>53.128395</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.452389</td>\n",
       "      <td>3.608671</td>\n",
       "      <td>0.379557</td>\n",
       "      <td>36.916973</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.307291</td>\n",
       "      <td>3.372669</td>\n",
       "      <td>0.413889</td>\n",
       "      <td>29.156248</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.221604</td>\n",
       "      <td>3.291163</td>\n",
       "      <td>0.422917</td>\n",
       "      <td>26.874105</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.131132</td>\n",
       "      <td>3.213343</td>\n",
       "      <td>0.436892</td>\n",
       "      <td>24.862070</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.047300</td>\n",
       "      <td>3.147552</td>\n",
       "      <td>0.447179</td>\n",
       "      <td>23.278996</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.978492</td>\n",
       "      <td>3.120242</td>\n",
       "      <td>0.455946</td>\n",
       "      <td>22.651857</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.921093</td>\n",
       "      <td>3.109544</td>\n",
       "      <td>0.458030</td>\n",
       "      <td>22.410818</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.876113</td>\n",
       "      <td>3.107162</td>\n",
       "      <td>0.458464</td>\n",
       "      <td>22.357492</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the Result\n",
    "\n",
    "With the fine-tuned model, we can run inference!\n",
    "We define the beginning of the content we want the model to output in `TEXT`, then we call `learn.predict` for the number of tokens we want the model to output and set temperature to determine the randomness/creativity of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEXT = \"I am interacting with a language model as a thought partner to\"\n",
    "N_WORDS = 256\n",
    "TEMP = 0.75\n",
    "pred = learn.predict(TEXT, N_WORDS, temperature=TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am interacting with a language model as a thought partner to train . \n",
      " I think it should be more difficult to use Sonnet but it is often difficult to get to because i am a very familiar language model . \n",
      "\n",
      " In this way , i wanted to use the phrase Sonnet > rather than just see the phrase as a word . \n",
      " He 's also using the word \" prompt \" , which is a word word that is used in art , then as a title to describe a language model that extract structured data from an individual 's experience . \n",
      " I 've used this idea to describe how a language model has a structure with a single structure and i can try and solve this with the following following Sonnet code : i n't have such a thead for a language model . \n",
      " Being like this is an easy way to design an [ initial Sonnet ] . \n",
      " In another example , where you would have written a single word to explicitly describe a language model , i had to use the following word usage : < inline_code > \n",
      " < / INLINE_CODE > : The word i used to read the script in a language model and then describe the word as an JSON object . \n",
      "\n",
      " This working was quite different from my JSON code and Python CODE > \n",
      "\n",
      " the most recent model to use this PYTHON code\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a little wild but it _kinda sorta_ makes sense and isn't too bad for a model I trained in a couple minutes on my MacBook.\n",
    "\n",
    "I tweaked several parts of the approach in effort to improve the model's output quality.\n",
    "Jumping back to the `clean_content` function, I found that removing the markdown frontmatter and replacing triple backticks with single tokens seemed to make the output make a bit more sense.\n",
    "When this fine-tuned model tries to generate code, it makes little sense and does strange things like emit tokens with triple words like `importimportimport`.\n",
    "I have a feeling this deficiency may be because the base model wasn't trained on much source code.\n",
    "\n",
    "So there we have it.\n",
    "A simple language model fine-tuned on my blog posts.\n",
    "This was a helpful experience for getting a feel for some feature engineering.\n",
    "\n",
    "If you liked this post, be sure to check out some of my other notebooks I've built while working through the FastAI Course linked below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
