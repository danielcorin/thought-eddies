---
date: "2024-01-10T19:10:18Z"
title: "2024-01-10"
draft: false
tags:
- cursor
- copilot
- language_models
---

After some experimentation with GitHub Copilot Chat, my review is mixed.
I like the ability to copy from the sidebar chat to the editor a lot.
It makes the chat more useful, but the chat is pretty chatty and thus somewhat slow to finish responding as a result.
I've also found the inline generation doesn't consistently respect instructions or highlighted context, which is probably the most common way I use Cursor, so that was a little disappointing.
To get similar behavior with Copilot, sometimes I needed to run a generation for the whole file, but the lack of specific highlighted context meant I had to write more specific instructions, which was more time-consuming than highlighting and giving shorter, more contextual instructions.
It is easy to edit the prompt and resubmit it if the completion is close, but not quite right, so that is helpful.

Maybe it's the learning curve of a new tool, but my intuition (given limited use so far) is I am fighting with Copilot Chat more than I do with Cursor.
The latter feels more like an extension of my editor whereas the former can be something I need to clean up after.
Overall, Copilot Chat is a good tool, with room for improvement.
There are UX features that I could imagine other code assistant tools adopting, like the copy-from-chat functionality.
I wonder how much the fact that I use the `gpt-4-1106-preview` model with Cursor, whereas Copilot uses the [Codex](https://openai.com/blog/openai-codex) model, impacts the output quality and ability to follow instructions with the same consistency.
It would be interesting to try Copilot with a different model to see how much of the differences are due to prompting and how much are due to the model capabilities.
