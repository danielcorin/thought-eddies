---
date: "2025-02-14T17:19:28Z"
title: "2025-02-14"
draft: false
tags:
---

I read Declan's [article](https://vale.rocks/posts/ai-is-stifling-tech-adoption) on new technology adoption and the problems posed by models.

I haven't had as much trouble as it seems Declan has in getting models to write vanilla js, but I share the concern about new technology/framework adoption and how we'll solve this problem if models end up authoring most of the code that gets written.

Maybe projects like llms.txt[^1] and similar can help by providing context to models on how to use a technology, but finding a way to do this is now a meaningful hurdle to using something new and the slop filled web seems to be making it harder to push forward the knowledge cutoff of the foundational models.

With regard to React/Tailwind specifically, I'm actually unsure if labs' preference for this stack is a result of models being good at wielding it due to their training or that models themselves (without a more complex architecture) are just better at writing and styling frontends with the structure and the style colocated in the same parts of the file as compared to different parts (e.g. css at the top, HTML/[T|J]SX and style class names at the bottom) or even two different files.
I wrote about this as well [^2].

Tools like Cursor's Composer agent are improving at multi-file edits, so if this limitation is genuine, it seems like it won't remain long-lived.

[^1]: https://llmstxt.org/
[^2]: https://www.thoughteddies.com/notes/2025/llm-tailwind-react
